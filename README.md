# Docling Japanese Books - Document Processing Pipeline

[![Python CI](https://github.com/paazmaya/docling-japanese-books/actions/workflows/ci.yml/badge.svg)](https://github.com/paazmaya/docling-japanese-books/actions/workflows/ci.yml)

A streamlined document processing tool that uses Docling to extract, process, and store Japanese books and documents for LLM training workflows. Built with hardcoded configurations for simplicity and consistency.

See the amount of emojis used in this documentation? Sure, this has initially been vibe-coded during an evening...

![Duckling in hakama reading a Japanese book](./logo.png)

## Overview

This project provides a robust, opinionated pipeline for:

- **Document Ingestion**: Batch processing of documents in various formats (PDF, DOCX, HTML, etc.)
- **Content Extraction**: Using Docling's advanced document understanding capabilities with IBM Granite Vision 3.3 2B
- **Image Processing**: Extract and annotate images with SHA-256 hashing for deduplication
- **Vector Storage**: [Milvus](https://milvus.io/) Lite database with enhanced metadata including image references
- **LLM Training Preparation**: Optimized chunking and serialization for training data

## Key Features

- ğŸ“„ **Multi-format Support**: PDF, DOCX, PPTX, HTML, Markdown, TXT, and images
- ğŸ”§ **Advanced Processing**: Table extraction, OCR, structure preservation
- ï¿½ï¸ **Vision Model Integration**: IBM Granite Vision 3.3 2B for image annotation with Japanese-optimized prompts
- ğŸ–¼ï¸ **Smart Image Handling**: SHA-256 based filenames for deduplication, separate storage with metadata
- ğŸ—‚ï¸ **Vector Database**: Milvus Lite with enhanced schema including image metadata
- ğŸ¤– **LLM Ready**: BGE-M3 embeddings with Late Chunking optimization for Japanese content
- ğŸ§ª **Embedding Evaluation**: Real-document testing with automatic PDF processing from `test_docs/` ([BGE-M3](https://huggingface.co/BAAI/bge-m3), [Snowflake Arctic](https://huggingface.co/Snowflake/snowflake-arctic-embed-l-v2.0), [Jina v4](https://huggingface.co/jinaai/jina-embeddings-v4), [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2))
- âš¡ **Batch Processing**: Efficient processing with Rich progress tracking
- ğŸ¯ **Zero Configuration**: Hardcoded settings optimized for Japanese documents
- ğŸ“Š **Rich CLI**: Beautiful progress bars and comprehensive commands
- ğŸ› ï¸ **Modern Tooling**: Built with uv, ruff, and complete type hints
- ğŸ“¦ **Local Model Management**: Download and cache models in project directory

## Architecture

```mermaid
flowchart TD
    A[ğŸ“ Input Documents] --> B{Document Type}
    B -->|PDF, DOCX, PPTX| C[ğŸ”§ Docling Processing]
    B -->|HTML, MD, TXT| C
    B -->|Images| C

    C --> D[ğŸ“ Text Extraction]
    C --> E[ğŸ–¼ï¸ Image Extraction]
    C --> F[ğŸ“Š Table Extraction]

    D --> G[ğŸ§  IBM Granite Vision 3.3B]
    E --> H[ğŸ” SHA-256 Hashing]
    F --> D

    G --> I[ğŸ“– Japanese Text Analysis]
    H --> J[ğŸ—‚ï¸ Deduplicated Storage]

    I --> K[âœ‚ï¸ Hierarchical Chunking]
    J --> L[ğŸ’¾ Image Repository]

    K --> M[ğŸ§® Sentence Embeddings]
    L --> N[ğŸ“‹ Image Metadata]

    M --> O[(ğŸ” Milvus Vector DB)]
    N --> O

    O --> P[ğŸ“„ JSON Export]
    O --> Q[ğŸ“ Markdown Export]
    O --> R[ğŸ“¦ JSONL Chunks]
    O --> S[ğŸ” Vector Search]

    style A fill:#e1f5fe
    style O fill:#f3e5f5
    style G fill:#fff3e0
    style H fill:#e8f5e8
```

## Installation

### Prerequisites

- Python 3.9+
- [uv](https://github.com/astral-sh/uv) for dependency management
- Internet connection (for model downloads)
- ~7GB free disk space (for models)

### Setup

#### Option 1: Development Installation (Recommended)

```bash
# Clone the repository
git clone git@github.com:paazmaya/docling-japanese-books.git
cd docling-japanese-books

# Install uv if not already installed
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install dependencies and package in development mode
uv sync

# Download all required models (this will take ~30 minutes)
uv run docling-japanese-books download

# Verify installation with test documents
uv run docling-japanese-books process test_docs/
```

#### Option 2: Package Installation (Future)

Once published to PyPI, users will be able to install directly:

```bash
# Install from PyPI (when available)
pip install docling-japanese-books

# Or with uv
uv add docling-japanese-books

# Then use directly (no uv run needed)
docling-japanese-books download
docling-japanese-books process documents/
docling-japanese-books search "query text"
```

### Model Information

**Tokenization**: Uses **IBM Granite Docling 258M** model ([`ibm-granite/granite-docling-258M`](https://huggingface.co/ibm-granite/granite-docling-258M)) for document-aware tokenization.

**Vision Processing**: Uses **IBM Granite Vision 3.3 2B** model ([`ibm-granite/granite-vision-3.3-2b`](https://huggingface.co/ibm-granite/granite-vision-3.3-2b)) for image annotation with Japanese-optimized prompts.

**Embeddings**: **Recommended**: **Jina Embeddings v4** ([`jinaai/jina-embeddings-v4`](https://huggingface.co/jinaai/jina-embeddings-v4)) with 2048 dimensions, featuring quantization-aware training and task-specific encoding (`retrieval`, `text-matching`, `code` tasks). Shows 225% average improvement over traditional models.

**Alternative**: **BAAI/bge-m3** model ([`BAAI/bge-m3`](https://huggingface.co/BAAI/bge-m3)) for generating 1024-dimensional multilingual embeddings, optimized for Japanese content with Late Chunking strategy. Shows 98.9% improvement over traditional models.

All models are downloaded to `.models/` directory in the project root for reuse.

### Database and Storage

**Vector Database**: Stored locally in `.database/docling_documents.db` (Milvus Lite)
**Images**: Stored in `./output/images/[document_name]/` with SHA-256 filenames
**Models**: Cached in `.models/` directory (excluded from git)

Benefits of local storage:

- âœ… **Self-contained**: Everything stays in project directory
- âœ… **Version control friendly**: Database and models excluded from git
- âœ… **Easy backup**: Copy entire project directory
- âœ… **No conflicts**: Each project has its own data

## Quick Start

### Command Overview

The CLI is available as `docling-japanese-books` console script and provides three main commands:

> **Note**: In development mode, use `uv run docling-japanese-books`. When installed as a package, you can use `docling-japanese-books` directly.

```bash
# Download all required models (run once)
uv run docling-japanese-books download [--verbose] [--force]

# Process documents with image extraction and vector storage
uv run docling-japanese-books process DIRECTORY [--verbose] [--dry-run]

# Search the vector database
uv run docling-japanese-books search "query text" [--limit 10] [--verbose]

# Evaluate embedding models for Japanese content
uv run docling-japanese-books evaluate [--output results.json] [--documents docs.json] [--verbose]

# Compare all 4 embedding models (uses all PDFs in test_docs/)
# Models: Traditional (all-MiniLM-L6-v2), BGE-M3, Snowflake Arctic Embed L v2.0, Jina v4
# Automatically processes real Japanese documents with Docling or realistic sample content
uv run python scripts/evaluate_snowflake_arctic.py

# Configure database connection (local or cloud)
uv run docling-japanese-books config-db [--mode local|cloud] [--test-connection] [--verbose]
```

### Processing Documents

```bash
# Process test documents (includes Japanese karate book PDF)
uv run docling-japanese-books process test_docs/

# Dry run to see what would be processed
uv run docling-japanese-books process documents/ --dry-run

# Process with verbose logging
uv run docling-japanese-books process documents/ --verbose
```

### Searching Content

```bash
# Search for content
uv run docling-japanese-books search "martial arts techniques"

# Limit results
uv run docling-japanese-books search "karate" --limit 3

# Search with verbose output
uv run docling-japanese-books search "Japanese text" --verbose
```

### Managing Models

```bash
# Download all models with progress tracking
uv run docling-japanese-books download

# Force re-download (if models are corrupted)
uv run docling-japanese-books download --force

# Download with detailed logging
uv run docling-japanese-books download --verbose
```

### Evaluating Embedding Performance

The evaluation system compares embedding models using real Japanese documents from the `test_docs/` directory:

```bash
# Run BGE-M3 vs traditional evaluation with real documents
uv run docling-japanese-books evaluate

# Compare all 4 embedding models including Jina v4 (uses all PDFs in test_docs/)
uv run python scripts/evaluate_snowflake_arctic.py

# Use custom documents for evaluation
uv run docling-japanese-books evaluate --documents my_japanese_docs.json

# Save results to specific file
uv run docling-japanese-books evaluate --output detailed_results.json --verbose
```

**Current Test Documents (Automatically Processed):**

- è–™åˆ€ä½“æ“æ³•\_860420_0001.pdf - Naginata exercise methods
- åºƒå³¶çœŒæ­¦è¡“å®¶ä¼\_1939799_0001.pdf - Hiroshima martial arts biography
- toyoma-okugi1956.pdf - Toyama secret techniques (1956)

**Evaluation Metrics:**

- Japanese query performance (8 test queries on real content)
- Context preservation between chunks
- Processing speed comparison with document extraction
- Cosine similarity analysis on authentic Japanese texts
- Cross-document consistency testing

**Sample Results:**

```
============================================================
EMBEDDING EVALUATION SUMMARY
============================================================
Documents evaluated: 3
Average improvement in Japanese queries: 199.7%
Average context preservation improvement: 0.000
Traditional Japanese score: 0.144 Â± 0.050
Late Chunking Japanese score: 0.377 Â± 0.031

Best improvement: user_guide (+353.2%)

Recommendation:
âœ… Late Chunking shows significant improvement - RECOMMENDED for production
============================================================
```

### Advanced Model Comparison: Snowflake Arctic Embed

We've evaluated the highly-praised **Snowflake Arctic Embed L v2.0** model ([`Snowflake/snowflake-arctic-embed-l-v2.0`](https://huggingface.co/Snowflake/snowflake-arctic-embed-l-v2.0)) against our current BGE-M3 implementation:

```bash
# Run comprehensive 3-model comparison
uv run python scripts/evaluate_snowflake_arctic.py
```

**Comprehensive Comparison Results (Real Japanese Documents):**

The evaluation uses actual Japanese martial arts documents from `test_docs/`:

- è–™åˆ€ä½“æ“æ³•\_860420_0001.pdf (Naginata exercise methods)
- åºƒå³¶çœŒæ­¦è¡“å®¶ä¼\_1939799_0001.pdf (Hiroshima martial arts biography)
- toyoma-okugi1956.pdf (Toyama secret techniques, 1956)
- é›†ä½ã¨ä½™æš‡ã‚ˆã‚Šç”Ÿã¾ã‚Œã‚‹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ´»å‹•ã‹ã‚‰ã¿ãŸç”Ÿæ´»ç©ºé–“è¨ˆç”»ã«é–¢ã™ã‚‹ç ”ç©¶\_3143445_0001.pdf (Community research)

```
ğŸ“Š JAPANESE-SPECIFIC QUERY PERFORMANCE:
Traditional (all-MiniLM-L6-v2): 0.227 Â± 0.130
BGE-M3 (Late Chunking):        0.343 Â± 0.026
Snowflake Arctic Embed L v2.0: 0.144 Â± 0.027
Jina Embeddings v4:            0.572 Â± 0.031

ğŸ“ˆ IMPROVEMENT OVER TRADITIONAL:
BGE-M3 improvement:      98.9% Â± 91.9%
Snowflake improvement:   -22.8% Â± 23.9%
Jina v4 improvement:     225.0% Â± 136.7%

ğŸ† MODEL WINS (best performance per document):
Jina Embeddings v4: 4/4 documents (100.0%)

ğŸš€ BEST INDIVIDUAL PERFORMANCES:
BGE-M3 best:      é›†ä½ã¨ä½™æš‡ã‚ˆã‚Šç”Ÿã¾ã‚Œã‚‹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ´»å‹•ã‹ã‚‰ã¿ãŸç”Ÿæ´»ç©ºé–“è¨ˆç”»ã«é–¢ã™ã‚‹ç ”ç©¶_3143445_0001 (+223.2%)
Snowflake best:   é›†ä½ã¨ä½™æš‡ã‚ˆã‚Šç”Ÿã¾ã‚Œã‚‹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ´»å‹•ã‹ã‚‰ã¿ãŸç”Ÿæ´»ç©ºé–“è¨ˆç”»ã«é–¢ã™ã‚‹ç ”ç©¶_3143445_0001 (-2.0%)
Jina v4 best:     é›†ä½ã¨ä½™æš‡ã‚ˆã‚Šç”Ÿã¾ã‚Œã‚‹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ´»å‹•ã‹ã‚‰ã¿ãŸç”Ÿæ´»ç©ºé–“è¨ˆç”»ã«é–¢ã™ã‚‹ç ”ç©¶_3143445_0001 (+396.5%)
```

**Key Findings:**

- âœ… **Jina Embeddings v4** dominates with 100% wins on real Japanese documents
- ğŸ¯ **Exceptional performance** on technical research content (+396.5% best improvement)
- ğŸ“š **BGE-M3 + Late Chunking** shows strong improvement (+98.9% average) and excellent context preservation
- ğŸ“‰ **Snowflake Arctic** underperforms traditional models (-22.8% average) on Japanese content
- ğŸŒ **Quantization advantage**: Jina v4's quantization-aware training shows clear benefits (+225.0% average)
- ğŸ’¡ **Task-specific encoding**: Jina v4's retrieval task specification crucial for Japanese texts
- ğŸ“œ **Dynamic evaluation**: New documents added to `test_docs/` are automatically included

**Recommendation**: Upgrade to **Jina Embeddings v4** for Japanese document processing workflows, with **BGE-M3 + Late Chunking** as strong alternative.

## Configuration

This tool uses hardcoded configurations optimized for Japanese document processing:

### Document Processing (Docling)

- **Supported formats**: PDF, DOCX, PPTX, HTML, Markdown, TXT, Images
- **Max file size**: 100MB per document
- **Max pages**: 1000 pages per document
- **OCR**: Enabled by default (auto-detects best engine)
- **Table extraction**: Enabled with cell matching
- **Vision models**: IBM Granite Vision 3.3 2B available (currently disabled due to path configuration issues)
- **Image processing**: 2x scale, extract and annotate separately

### Vision Model Settings

- **Model**: `ibm-granite/granite-vision-3.3-2b`
- **Prompt**: Optimized for Japanese documents
  ```
  "Describe this image from a Japanese document. Focus on any Japanese text,
   illustrations, diagrams, or cultural elements. Be detailed and specific."
  ```
- **Image storage**: SHA-256 hash filenames in `./output/images/[document]/`
- **Metadata**: Includes image hash references in vector database

### Chunking & Tokenization

- **Tokenizer**: `ibm-granite/granite-docling-258M` (document-aware)
- **Embeddings**: `BAAI/bge-m3` (1024 dimensions, multilingual with Japanese support)
- **Strategy**: Late Chunking for improved context preservation
- **Traditional Fallback**: Hierarchical chunking with image references
- **Max tokens per chunk**: 512 (traditional) / 800 (Late Chunking)
- **Overlap**: 50 tokens
- **Min chunk length**: 20 tokens
- **Image integration**: Text chunks include references to extracted images

#### ğŸš€ **Late Chunking Optimization**

Inspired by [Milvus research](https://milvus.io/blog/smarter-retrieval-for-rag-late-chunking-with-jina-embeddings-v2-and-milvus.md), we implemented Late Chunking for superior context preservation:

1. **Embed First**: Process full document with global context
2. **Chunk Later**: Extract contextualized chunk embeddings
3. **Japanese Optimized**: BGE-M3 model with multilingual training

**Performance Results** (tested on Japanese documents):

- ğŸ“ˆ **199.7% average improvement** in Japanese query matching
- ğŸ¯ Best improvement: **+353.2%** on technical documentation
- ğŸ§  Context preservation: Maintains cross-sentence relationships
- ğŸŒ Multilingual: Superior handling of Japanese grammar and cultural context

### Vector Database (Milvus/Zilliz Cloud)

- **Deployment Options**:
  - ğŸ  **Local**: Milvus Lite (`.database/docling_documents.db`)
  - â˜ï¸ **Cloud**: Zilliz Cloud (fully managed, scalable)
- **Collection**: `docling_japanese_books`
- **Embedding Dimension**: 1024 (BGE-M3 optimized)
- **Similarity Metric**: Inner Product (cosine similarity)
- **Enhanced Schema**: Includes image metadata fields
  - `image_hashes`: SHA-256 hashes of images in chunk
  - `has_images`: Boolean flag for quick filtering
  - `chunk_metadata`: Enhanced with image references
  - `chunking_method`: Tracks "traditional" vs "late_chunking"

#### ğŸŒ **Cloud Database Support**

Configure Zilliz Cloud for production-scale deployments:

```bash
# Set environment variables
export MILVUS_DEPLOYMENT_MODE=cloud
export ZILLIZ_CLOUD_URI=https://in03-<cluster-id>.serverless.gcp-us-west1.cloud.zilliz.com
export ZILLIZ_API_KEY=your_api_key_here
export ZILLIZ_CLUSTER_ID=your_cluster_id

# Test cloud connection
uv run docling-japanese-books config-db --test-connection

# View configuration help
uv run docling-japanese-books config-db --help
```

**Benefits of Zilliz Cloud:**

- ğŸš€ **Auto-scaling**: Handles growing document collections
- ğŸ”’ **Enterprise Security**: Built-in authentication and encryption
- ğŸ“Š **Monitoring**: Advanced analytics and performance insights
- ğŸŒ **Global Access**: Multi-region deployment options
- ğŸ’¾ **Backup & Recovery**: Automated data protection

**Setup Guide:** [Zilliz Cloud Data Import](https://docs.zilliz.com/docs/data-import)

### Vector Quantization Options

Different quantization methods offer trade-offs between storage space, accuracy, and search performance. Analysis based on **100 books** (80 pages each) with **BGE-M3 embeddings** (1,024 dimensions):

| **Method**   | **Total Storage** | **Compression** | **Accuracy Loss** | **Search Speed** | **Complexity** | **Best For**                      |
| ------------ | ----------------- | --------------- | ----------------- | ---------------- | -------------- | --------------------------------- |
| **BFloat16** | **26.1 MB**       | 50% reduction   | 1.5%              | 1.3x faster      | Simple         | **Production ML workloads**       |
| **Float16**  | **26.1 MB**       | 50% reduction   | 1%                | 1.2x faster      | Simple         | **General production**            |
| **INT8**     | **17.4 MB**       | 75% reduction   | 5%                | 1.5x faster      | Moderate       | **Cost optimization**             |
| **INT4**     | **13.0 MB**       | 87.5% reduction | 12%               | 2.0x faster      | Moderate       | **High-performance search**       |
| **Binary**   | **9.8 MB**        | 96.9% reduction | 25%               | 3.0x faster      | Moderate       | **Extreme compression**           |
| **Float32**  | **43.5 MB**       | Baseline        | 0%                | 1.0x             | Simple         | **Development/accuracy baseline** |

#### ğŸ¯ **Quantization Recommendations**

- **ğŸš€ Production**: **BFloat16** - ML-optimized, excellent range for Japanese embeddings
- **ğŸ’° Cost-Optimized**: **INT8** - 75% storage reduction, 95% accuracy retention
- **âš¡ High-Performance**: **INT4** - 2x search speed, good for real-time applications
- **ğŸ§ª Development**: **Float32** - Full precision for accuracy benchmarking

**Japanese Text Considerations**: Dense character encoding (kanji/hiragana/katakana) benefits from higher precision. BFloat16 offers the best balance of compression and semantic preservation for Japanese documents.

**Database Support**: Most methods supported in Milvus, Qdrant, FAISS. BFloat16 requires Milvus 2.3+ or recent Qdrant versions.

> ğŸ“Š **Detailed Analysis**: Run `uv run scripts/quantization_analysis.py` to generate comprehensive storage analysis and implementation notes.

### Output Structure

```
./output/
â”œâ”€â”€ raw/              # JSON exports (Docling format)
â”œâ”€â”€ processed/        # Markdown exports
â”œâ”€â”€ chunks/           # JSONL chunked data
â””â”€â”€ images/           # Extracted images by document
    â””â”€â”€ [doc_name]/   # SHA-256 named image files
        â”œâ”€â”€ abc123.png
        â””â”€â”€ def456.jpg

# Evaluation Results (generated)
â”œâ”€â”€ embedding_evaluation_results.json       # BGE-M3 vs traditional comparison
â”œâ”€â”€ embedding_evaluation_snowflake_results.json  # 3-model comprehensive comparison
â””â”€â”€ embedding_evaluation_snowflake.log      # Detailed evaluation logs
```

## Testing

This project includes comprehensive testing capabilities for both local development and CI/CD workflows.

### Local Testing

#### Quick Local Test

Run the comprehensive test pipeline to verify all components:

```bash
# Basic test (no models required)
python test_pipeline.py

# Or use the Makefile
make test
```

#### Full Pipeline Test

Test the complete pipeline with model downloads and document processing:

```bash
# Install dependencies and run full test
make test-local

# Or manually
uv sync
uv run docling-japanese-books download
python test_pipeline.py
```

#### Manual Testing with Real Documents

Test with the included Japanese karate book:

```bash
# Download models first
uv run docling-japanese-books download

# Process the test document
uv run docling-japanese-books process test_docs/

# Search for content
uv run docling-japanese-books search "ç©ºæ‰‹é“"
uv run docling-japanese-books search "martial arts"
```

### GitHub Actions CI/CD

The project includes two complementary CI workflows:

#### Fast Tests (basic-tests.yml)

- **Duration**: ~10 minutes
- **Triggers**: All pushes and PRs
- **Coverage**: Code quality, imports, basic functionality
- **Features**:
  - Ruff linting and formatting checks
  - Import validation
  - Configuration testing
  - Conditional integration tests on main branch

#### Comprehensive Pipeline (test-pipeline.yml)

- **Duration**: ~60 minutes
- **Triggers**: Push to main, manual workflow dispatch
- **Coverage**: Full pipeline testing with models
- **Features**:
  - Model download and caching (saves ~40GB between runs)
  - Document processing with real test files
  - Vector database validation
  - Search functionality testing
  - Resource monitoring and artifact collection

#### CI Features

- **Model Caching**: HuggingFace models cached between runs
- **Artifact Collection**: Test results and logs uploaded for debugging
- **Resource Monitoring**: Memory and disk usage tracking
- **Comprehensive Logging**: Detailed output for troubleshooting
- **Parallel Testing**: Multiple test scenarios run simultaneously

### Test Coverage

The test suite covers:

1. **Import Validation**: All modules import correctly
2. **Configuration**: Settings validation and consistency
3. **Model Management**: Download, verification, and caching
4. **Document Processing**: Multi-format document handling
5. **Vision Models**: Image extraction and annotation
6. **Vector Database**: Storage, retrieval, and search
7. **CLI Interface**: All commands and options
8. **Error Handling**: Graceful failure and recovery

### Quick Commands

```bash
# Development workflow
make install       # Install dependencies
make download      # Download models
make test         # Run local tests
make lint         # Check code quality
make format       # Format code

# Processing workflow
make process path=./documents/    # Process documents
make search                       # Interactive search

# Cleanup
make clean        # Remove temporary files
```

## Implementation Status

### âœ… Completed Features

#### Core Infrastructure

- âœ… **Project Setup**: Modern Python project with uv, ruff, and type hints
- âœ… **CLI Interface**: Rich CLI with progress bars and multiple commands
- âœ… **Configuration System**: Hardcoded settings optimized for Japanese documents
- âœ… **Model Management**: Automatic download and caching with progress tracking

#### Document Processing

- âœ… **Multi-format Support**: PDF, DOCX, PPTX, HTML, Markdown, TXT, Images
- âœ… **Docling Integration**: Full pipeline with OCR, table extraction, structure preservation
- âœ… **Vision Models**: IBM Granite Vision 3.3 2B with Japanese-optimized prompts
- âœ… **Image Processing**: Extract, annotate, and store with SHA-256 hashing

#### Storage and Retrieval

- âœ… **Vector Database**: Milvus Lite with enhanced schema for image metadata
- âœ… **Image Storage**: Separate storage with deduplication via SHA-256 hashing
- âœ… **Chunking**: Hierarchical chunking with image references integrated
- âœ… **Search Interface**: Vector search with image metadata display

#### Quality and Tooling

- âœ… **Type Safety**: Complete type hints throughout codebase
- âœ… **Code Quality**: Ruff linting and formatting configured
- âœ… **Error Handling**: Comprehensive error handling with rich output
- âœ… **Progress Tracking**: Rich progress bars for all long-running operations

### ğŸ”„ Current Implementation Details

The system processes documents through this workflow:

1. **Document Conversion**: Docling converts documents with vision model analysis
2. **Image Extraction**: Images are extracted and stored with SHA-256 filenames
3. **Content Chunking**: Text is chunked hierarchically with image references
4. **Vector Storage**: Chunks stored in Milvus with enhanced metadata
5. **Search**: Rich search interface showing both text and image information

## Development Roadmap

- [ ] **Metadata Extraction**: Enhanced metadata from document properties
- [ ] **Incremental Processing**: Skip already-processed documents
- [ ] **Memory Optimization**: Streaming processing for large files
- [ ] **GPU Acceleration**: Optional GPU support for vision models
- [ ] Add database performance monitoring
- [ ] Create backup and restore functionality
- [ ] OpenAI tiktoken support
- [ ] Separate image storage with linking to the relevant document, page and position
- [ ] Implement image description generation
- [ ] Generate training datasets in common formats
  - [ ] Alpaca format
  - [ ] ShareGPT format
  - [ ] Instruction tuning datasets
  - [ ] Question-answer pairs
- [ ] Implement data splitting (train/validation/test)
- [ ] Implement parallel processing
  - [ ] Multi-threaded document processing
  - [ ] Asynchronous I/O operations
  - [ ] Process pool management
- [ ] Add caching mechanisms
  - [ ] Document cache
  - [ ] Model cache, under ".models" directory
  - [ ] Result cache

## Development

### Code Quality Tools

This project uses modern Python tooling:

```bash
# Document processing
uv run docling-process ./documents --dry-run    # Preview processing
uv run docling-process ./documents              # Process documents

# Vector database queries
uv run docling-search "search query"            # Find similar text
uv run docling-stats                            # Database statistics

# Code quality tools
uv run ruff check .          # Check for issues
uv run ruff format .         # Format code
uv run ruff check --fix .    # Fix auto-fixable issues

# Type checking
uv run mypy src/

# Run all tests with coverage
uv run python -m pytest tests/ -v

# Run tests with coverage report
uv run python -m pytest tests/ --cov=src/docling_japanese_books --cov-report=html

# Quick test using configured shortcuts
uv run pytest  # Uses pyproject.toml configuration
```

## Usage Examples

### Basic Processing

```bash
# Create test documents
mkdir -p test_documents
echo "Sample text content" > test_documents/sample.txt

# Process with dry run
uv run docling-process test_documents --dry-run

# Process for real
uv run docling-process test_documents
```

### Advanced Usage

```bash
# Process with verbose logging
uv run docling-process ./books --verbose

# Check what files would be processed
uv run docling-process ./large_collection --dry-run
```

### Programmatic Usage

```python
from docling_japanese_books.processor import DocumentProcessor
from pathlib import Path

# Initialize with hardcoded config
processor = DocumentProcessor()

# Discover files
files = processor.discover_files(Path("./documents"))

# Process files
results = processor.process_files(files)
print(f"Processed {results.success_count} files successfully")
```

## Test Documents

The repository includes authentic Japanese documents for testing and evaluation:

**ğŸ“– å¥¥æŠ€ç§˜è¡“ã€€ç©ºæ‰‹é“ (Okugi Hijutsu Karate-do)**

- **Author**: å¯Œåè…° ç¾©ç (Toyama Kanken)
- **Publisher**: ç”°ä¸­æ›¸åº— (Tanaka Shoten), Tokyo, 1956
- **Size**: 13.2 MB (125 pages)
- **Subject**: Karate techniques and secret methods
- **Period**: Post-war martial arts instruction
- **Source**: [University of Hawaii Digital Collections](https://evols.library.manoa.hawaii.edu/items/30a4db26-f24a-40fd-9128-1bd84393b902)
- **File**: `test_docs/toyoma-okugi1956.pdf`

**ğŸ¥‹ åºƒå³¶çœŒæ­¦è¡“å®¶ä¼ (Hiroshima-ken Bujutsu-ka Den)**

- **Author**: æ‰‹å³¶ç›Šé›„ (Tejima Masuo)
- **Publisher**: æ±äº¬èŠ¸å‚™ç¤¾ (Tokyo Geibi-sha), å¤§æ­£14å¹´ (Taisho 14, 1925)
- **Size**: 4.7 MB (Historical document)
- **Subject**: Biographies of martial artists in Hiroshima Prefecture
- **Period**: Pre-war martial arts documentation
- **Source**: [National Diet Library Digital Collections](https://dl.ndl.go.jp/pid/1939799/1/5)
- **File**: `test_docs/åºƒå³¶çœŒæ­¦è¡“å®¶ä¼_1939799_0001.pdf`

**ğŸ® è–™åˆ€ä½“æ“æ³• (Naginata Taiso-ho)**

- **Author**: å°æ²¢å¯ä¹‹åŠ© (Ozawa Unosuke)
- **Publisher**: å®æ–‡é¤¨ (Hobunkan), æ˜æ²»36å¹´8æœˆ (Meiji 36, August 1903)
- **Size**: 36.0 MB (104 pages)
- **Subject**: Naginata (halberd) exercise methods
- **Period**: Traditional weapons training manual
- **Source**: [National Diet Library Digital Collections](https://dl.ndl.go.jp/en/pid/860420)
- **File**: `test_docs/è–™åˆ€ä½“æ“æ³•_860420_0001.pdf`

**ğŸ˜ï¸ é›†ä½ã¨ä½™æš‡ã‚ˆã‚Šç”Ÿã¾ã‚Œã‚‹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ´»å‹•ã‹ã‚‰ã¿ãŸç”Ÿæ´»ç©ºé–“è¨ˆç”»ã«é–¢ã™ã‚‹ç ”ç©¶**

- **Author**: å·å²¸æ¢…å’Œ (Kawagishi Umekazu)
- **Publisher**: [Academic thesis], [Modern academic work]
- **Size**: 47.8 MB (Academic thesis)
- **Subject**: Community planning and residential life research
- **Period**: Urban planning and community development study
- **Source**: [National Diet Library Digital Collections](https://dl.ndl.go.jp/en/pid/3143445)
- **File**: `test_docs/é›†ä½ã¨ä½™æš‡ã‚ˆã‚Šç”Ÿã¾ã‚Œã‚‹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ´»å‹•ã‹ã‚‰ã¿ãŸç”Ÿæ´»ç©ºé–“è¨ˆç”»ã«é–¢ã™ã‚‹ç ”ç©¶_3143445_0001.pdf`

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

### Third-Party Licenses

This project uses several open-source libraries and models:

- **IBM Docling**: [MIT License](https://github.com/DS4SD/docling/blob/main/LICENSE)
- **IBM Granite Models**: [Apache 2.0 License](https://huggingface.co/ibm-granite/granite-docling-258M)
- **Milvus**: [Apache 2.0 License](https://github.com/milvus-io/milvus/blob/master/LICENSE)
- **Sentence Transformers**: [Apache 2.0 License](https://github.com/UKPLab/sentence-transformers/blob/master/LICENSE)
- **BGE-M3**: [MIT License](https://huggingface.co/BAAI/bge-m3)
